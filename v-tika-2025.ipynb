{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12977119,"sourceType":"datasetVersion","datasetId":7794557}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"fd96fd5b-d951-4867-9316-4e18d695fef8","cell_type":"code","source":"!pip install rouge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:20:49.984114Z","iopub.execute_input":"2025-09-20T13:20:49.984280Z","iopub.status.idle":"2025-09-20T13:20:55.420547Z","shell.execute_reply.started":"2025-09-20T13:20:49.984257Z","shell.execute_reply":"2025-09-20T13:20:55.419578Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}],"execution_count":1},{"id":"c1d69dba-f0d6-48dd-ba86-5c868c516c02","cell_type":"code","source":"import json\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForQuestionAnswering,\n    TrainingArguments,\n    Trainer,\n    default_data_collator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:20:55.421638Z","iopub.execute_input":"2025-09-20T13:20:55.421929Z","iopub.status.idle":"2025-09-20T13:21:33.330198Z","shell.execute_reply.started":"2025-09-20T13:20:55.421903Z","shell.execute_reply":"2025-09-20T13:21:33.329534Z"}},"outputs":[{"name":"stderr","text":"2025-09-20 13:21:16.477588: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758374476.800534      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758374476.893933      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"id":"c47c49cf-71a0-42f1-8dff-3c5490d09580","cell_type":"code","source":"def load_vatika_data(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    contexts, questions, answers = [], [], []\n    for domain in data[\"domains\"]:\n        for item in domain[\"contexts\"]:\n            context = item[\"context\"]\n            for qa in item[\"qas\"]:\n                contexts.append(context)\n                questions.append(qa[\"question\"])\n                answers.append(qa[\"answer\"])\n    return {\"context\": contexts, \"question\": questions, \"answers\": answers}\n\ntrain_path = \"/kaggle/input/vatika/train.json\"\nval_path = \"/kaggle/input/vatika/validation.json\"\ntestA_path = \"/kaggle/input/vatika/test-A-gold.json\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:21:33.331470Z","iopub.execute_input":"2025-09-20T13:21:33.332059Z","iopub.status.idle":"2025-09-20T13:21:33.337168Z","shell.execute_reply.started":"2025-09-20T13:21:33.332038Z","shell.execute_reply":"2025-09-20T13:21:33.336560Z"}},"outputs":[],"execution_count":3},{"id":"54fdda48-83ff-47d5-a3d8-47bb871b2d0e","cell_type":"code","source":"train_data = load_vatika_data(train_path)\nval_data = load_vatika_data(val_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:39:11.036207Z","iopub.execute_input":"2025-09-20T13:39:11.036775Z","iopub.status.idle":"2025-09-20T13:39:11.382369Z","shell.execute_reply.started":"2025-09-20T13:39:11.036748Z","shell.execute_reply":"2025-09-20T13:39:11.381787Z"}},"outputs":[],"execution_count":4},{"id":"e009571c-3ade-4e4a-a978-6b0d8b256b5f","cell_type":"code","source":"def prepare_features(examples, tokenizer):\n    tokenized = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=512,\n        stride=128,\n        return_offsets_mapping=True,\n        padding=\"max_length\"\n    )\n    start_positions, end_positions = [], []\n    for i, offsets in enumerate(tokenized[\"offset_mapping\"]):\n        context = examples[\"context\"][i]\n        answer = examples[\"answers\"][i]\n        answer_start = context.find(answer)\n        answer_end = answer_start + len(answer)\n        start = end = 0\n        for idx, (s, e) in enumerate(offsets):\n            if s <= answer_start < e:\n                start = idx\n            if s < answer_end <= e:\n                end = idx\n                break\n        start_positions.append(start)\n        end_positions.append(end)\n    tokenized[\"start_positions\"] = start_positions\n    tokenized[\"end_positions\"] = end_positions\n    return tokenized\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"google/muril-base-cased\")\n\ndataset = Dataset.from_dict(train_data)\nval_dataset = Dataset.from_dict(val_data)\n\ntokenized_dataset = dataset.map(lambda x: prepare_features(x, tokenizer), batched=True)\ntokenized_val_dataset = val_dataset.map(lambda x: prepare_features(x, tokenizer), batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:39:12.136658Z","iopub.execute_input":"2025-09-20T13:39:12.137306Z","iopub.status.idle":"2025-09-20T13:39:40.124367Z","shell.execute_reply.started":"2025-09-20T13:39:12.137279Z","shell.execute_reply":"2025-09-20T13:39:40.123770Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f346d5a20a2844fc8899e6386c5281c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d30bd564754d5fae47fdac61188484"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cf8983ad7084f94b1ec177730752b5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ed25bfac745412cbb0369ffdf131f74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12724b8b2a3d44e2b9b5b72f0933fe63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07530ff1901d44c8af0ac5dd55a4f5db"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13092 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f377742a8e9649c7a24114457e11b183"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2798 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c8adf6e9a134649947244e2b5bd57ad"}},"metadata":{}}],"execution_count":5},{"id":"2942cd0f-9517-4d7e-98dd-b55f62771e2d","cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./muril-vatika\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=8,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    save_strategy=\"no\",\n    logging_strategy=\"no\",         \n    report_to=\"none\",              \n    disable_tqdm=True              \n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer,\n    data_collator=default_data_collator\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:39:40.125720Z","iopub.execute_input":"2025-09-20T13:39:40.125962Z","iopub.status.idle":"2025-09-20T14:11:01.504631Z","shell.execute_reply.started":"2025-09-20T13:39:40.125944Z","shell.execute_reply":"2025-09-20T14:11:01.503845Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3999992931.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'train_runtime': 1880.1566, 'train_samples_per_second': 13.926, 'train_steps_per_second': 0.871, 'train_loss': 4.869628011961996, 'epoch': 2.0}\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1638, training_loss=4.869628011961996, metrics={'train_runtime': 1880.1566, 'train_samples_per_second': 13.926, 'train_steps_per_second': 0.871, 'train_loss': 4.869628011961996, 'epoch': 2.0})"},"metadata":{}}],"execution_count":6},{"id":"9ac5b03d-9ac6-4334-8a0e-e3cb6e9720d2","cell_type":"code","source":"def compute_metrics(references, predictions):\n    # F1\n    f1s = []\n    for ref, pred in zip(references, predictions):\n        ref_tokens, pred_tokens = ref.split(), pred.split()\n        common = set(ref_tokens) & set(pred_tokens)\n        if not common:\n            f1s.append(0)\n        else:\n            precision = len(common) / len(pred_tokens) if pred_tokens else 0\n            recall = len(common) / len(ref_tokens) if ref_tokens else 0\n            if precision + recall == 0:\n                f1s.append(0)\n            else:\n                f1s.append(2 * precision * recall / (precision + recall))\n    avg_f1 = sum(f1s) / len(f1s)\n\n    # Exact Match\n    em = sum([1 if r.strip() == p.strip() else 0 for r, p in zip(references, predictions)]) / len(references)\n\n    # BLEU\n    bleu = sum([sentence_bleu([r.split()], p.split()) for r, p in zip(references, predictions)]) / len(references)\n\n    # ROUGE-L (skip empty cases)\n    rouge = Rouge()\n    rouge_scores = []\n    for r, p in zip(references, predictions):\n        if not p.strip() or not r.strip():   \n            rouge_scores.append(0)\n            continue\n        score = rouge.get_scores(p, r)[0]['rouge-l']['f']\n        rouge_scores.append(score)\n    rouge_l = sum(rouge_scores) / len(rouge_scores)\n\n    return avg_f1, em, bleu, rouge_l","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:11:01.505606Z","iopub.execute_input":"2025-09-20T14:11:01.505899Z","iopub.status.idle":"2025-09-20T14:11:01.513272Z","shell.execute_reply.started":"2025-09-20T14:11:01.505875Z","shell.execute_reply":"2025-09-20T14:11:01.512721Z"}},"outputs":[],"execution_count":7},{"id":"15dfb213-984e-43e0-8501-89799dd4e7a9","cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfrom rouge import Rouge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:11:01.514548Z","iopub.execute_input":"2025-09-20T14:11:01.514740Z","iopub.status.idle":"2025-09-20T14:11:02.677188Z","shell.execute_reply.started":"2025-09-20T14:11:01.514725Z","shell.execute_reply":"2025-09-20T14:11:02.676564Z"}},"outputs":[],"execution_count":8},{"id":"a5e4e033-9c30-4376-ba2e-98f546b5d7dc","cell_type":"code","source":"def predict_answers(model, tokenizer, data):\n    model.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    preds, refs = [], []\n    for domain in data[\"domains\"]:\n        for ctx in domain[\"contexts\"]:\n            context = ctx[\"context\"]\n            for qa in ctx[\"qas\"]:\n                question = qa[\"question\"]\n                refs.append(qa[\"answer\"])\n                inputs = tokenizer(\n                    question,\n                    context,\n                    return_tensors=\"pt\",\n                    truncation=True,\n                    max_length=512,\n                    padding=\"max_length\"\n                ).to(device)\n                with torch.no_grad():\n                    outputs = model(**inputs)\n                    start = torch.argmax(outputs.start_logits)\n                    end = torch.argmax(outputs.end_logits)\n                    if start > end:  # fallback fix\n                        start, end = end, start\n                    tokens = inputs[\"input_ids\"][0][start:end+1]\n                    answer = tokenizer.decode(tokens, skip_special_tokens=True).strip()\n                    if not answer:  # avoid empty prediction\n                        answer = \"[UNK]\"\n                    preds.append(answer)\n    return refs, preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:11:02.677906Z","iopub.execute_input":"2025-09-20T14:11:02.678129Z","iopub.status.idle":"2025-09-20T14:11:02.684187Z","shell.execute_reply.started":"2025-09-20T14:11:02.678112Z","shell.execute_reply":"2025-09-20T14:11:02.683490Z"}},"outputs":[],"execution_count":9},{"id":"b388cc29-c511-4c3f-adfc-28f14bf6d46e","cell_type":"code","source":"\nval_data_json = json.load(open(val_path, 'r', encoding='utf-8'))\nval_refs, val_preds = predict_answers(model, tokenizer, val_data_json)\nval_f1, val_em, val_bleu, val_rouge = compute_metrics(val_refs, val_preds)\n\nprint(\"\\nValidation Metrics\")\nprint(f\"F1 Score: {val_f1:.4f}\")\nprint(f\"Exact Match: {val_em:.4f}\")\nprint(f\"BLEU Score: {val_bleu:.4f}\")\nprint(f\"ROUGE-L Score: {val_rouge:.4f}\")\n\n\ntestA_data_json = json.load(open(testA_path, 'r', encoding='utf-8'))\ntestA_refs, testA_preds = predict_answers(model, tokenizer, testA_data_json)\ntestA_f1, testA_em, testA_bleu, testA_rouge = compute_metrics(testA_refs, testA_preds)\n\nprint(\"\\n Test-A Metrics\")\nprint(f\"F1 Score: {testA_f1:.4f}\")\nprint(f\"Exact Match: {testA_em:.4f}\")\nprint(f\"BLEU Score: {testA_bleu:.4f}\")\nprint(f\"ROUGE-L Score: {testA_rouge:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:11:02.684931Z","iopub.execute_input":"2025-09-20T14:11:02.685177Z","iopub.status.idle":"2025-09-20T14:14:23.706594Z","shell.execute_reply.started":"2025-09-20T14:11:02.685156Z","shell.execute_reply":"2025-09-20T14:14:23.705986Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 4-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 3-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 2-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"\nValidation Metrics\nF1 Score: 0.5088\nExact Match: 0.0000\nBLEU Score: 0.3513\nROUGE-L Score: 0.5333\n\n Test-A Metrics\nF1 Score: 0.4962\nExact Match: 0.0000\nBLEU Score: 0.3382\nROUGE-L Score: 0.5201\n","output_type":"stream"}],"execution_count":10},{"id":"89eea1e9-9c0c-483b-8237-464e6a924929","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}