{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12977119,"sourceType":"datasetVersion","datasetId":7794557}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T12:31:17.563833Z","iopub.execute_input":"2025-09-20T12:31:17.564686Z","iopub.status.idle":"2025-09-20T12:31:23.575457Z","shell.execute_reply.started":"2025-09-20T12:31:17.564647Z","shell.execute_reply":"2025-09-20T12:31:23.574745Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import json\nimport torch\nfrom datasets import Dataset\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForQuestionAnswering,\n    TrainingArguments,\n    Trainer,\n    default_data_collator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T12:31:23.577093Z","iopub.execute_input":"2025-09-20T12:31:23.577338Z","iopub.status.idle":"2025-09-20T12:32:04.708998Z","shell.execute_reply.started":"2025-09-20T12:31:23.577315Z","shell.execute_reply":"2025-09-20T12:32:04.708134Z"}},"outputs":[{"name":"stderr","text":"2025-09-20 12:31:46.746738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758371507.133015      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758371507.228730      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def load_vatika_data(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    contexts, questions, answers = [], [], []\n    for domain in data[\"domains\"]:\n        for item in domain[\"contexts\"]:\n            context = item[\"context\"]\n            for qa in item[\"qas\"]:\n                contexts.append(context)\n                questions.append(qa[\"question\"])\n                answers.append(qa[\"answer\"])\n    return {\"context\": contexts, \"question\": questions, \"answers\": answers}\n\ntrain_path = \"/kaggle/input/vatika/train.json\"\ntrain_data = load_vatika_data(train_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T12:32:04.709885Z","iopub.execute_input":"2025-09-20T12:32:04.710435Z","iopub.status.idle":"2025-09-20T12:32:04.941905Z","shell.execute_reply.started":"2025-09-20T12:32:04.710413Z","shell.execute_reply":"2025-09-20T12:32:04.941328Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def prepare_features(examples, tokenizer):\n    tokenized = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=512,\n        stride=128,\n        return_offsets_mapping=True,\n        padding=\"max_length\"\n    )\n    start_positions, end_positions = [], []\n    for i, offsets in enumerate(tokenized[\"offset_mapping\"]):\n        context = examples[\"context\"][i]\n        answer = examples[\"answers\"][i]\n        answer_start = context.find(answer)\n        answer_end = answer_start + len(answer)\n\n        start = end = 0\n        for idx, (s, e) in enumerate(offsets):\n            if s <= answer_start < e:\n                start = idx\n            if s < answer_end <= e:\n                end = idx\n                break\n        start_positions.append(start)\n        end_positions.append(end)\n\n    tokenized[\"start_positions\"] = start_positions\n    tokenized[\"end_positions\"] = end_positions\n    return tokenized\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"google/muril-base-cased\")\n\ndataset = Dataset.from_dict(train_data)\ntokenized_dataset = dataset.map(lambda x: prepare_features(x, tokenizer), batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T12:32:04.943296Z","iopub.execute_input":"2025-09-20T12:32:04.943559Z","iopub.status.idle":"2025-09-20T12:32:29.498911Z","shell.execute_reply.started":"2025-09-20T12:32:04.943518Z","shell.execute_reply":"2025-09-20T12:32:29.498035Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d77c13f2752942ebb173396939fdb0a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d978ef2248f045398f1fad65f69ff5aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb1f739b6a6c47009b32ab1f7b2838cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b51f6cec4b64cb2935cd8f1b0631028"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3427d3c167f64ecaac73559de376afd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"424c83cd8abb4eab95bec94706dbc3e1"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13092 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56fd8e4cfbc442c090d4fe8f7e4f69b4"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./muril-vatika\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=8,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    save_strategy=\"no\",\n    logging_strategy=\"no\",\n    report_to=\"none\",\n    disable_tqdm=True\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer,\n    data_collator=default_data_collator\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T12:32:29.499944Z","iopub.execute_input":"2025-09-20T12:32:29.500162Z","iopub.status.idle":"2025-09-20T13:01:34.298653Z","shell.execute_reply.started":"2025-09-20T12:32:29.500144Z","shell.execute_reply":"2025-09-20T13:01:34.297851Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/891834965.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'train_runtime': 1743.8363, 'train_samples_per_second': 15.015, 'train_steps_per_second': 0.939, 'train_loss': 4.831488119085775, 'epoch': 2.0}\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1638, training_loss=4.831488119085775, metrics={'train_runtime': 1743.8363, 'train_samples_per_second': 15.015, 'train_steps_per_second': 0.939, 'train_loss': 4.831488119085775, 'epoch': 2.0})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def load_test(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        return json.load(f)\n\ndef predict_answers(model, tokenizer, test_data):\n    model.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    for domain in test_data[\"domains\"]:\n        for ctx in domain[\"contexts\"]:\n            context = ctx[\"context\"]\n            for qa in ctx[\"qas\"]:\n                question = qa[\"question\"]\n                inputs = tokenizer(\n                    question,\n                    context,\n                    return_tensors=\"pt\",\n                    truncation=True,\n                    max_length=512,\n                    padding=\"max_length\"\n                ).to(device)\n\n                with torch.no_grad():\n                    outputs = model(**inputs)\n                    start = torch.argmax(outputs.start_logits)\n                    end = torch.argmax(outputs.end_logits)\n                    tokens = inputs[\"input_ids\"][0][start:end+1]\n                    answer = tokenizer.decode(tokens, skip_special_tokens=True)\n                    qa[\"answer\"] = answer.strip()\n    return test_data\n\ntest_path = \"/kaggle/input/vatika/test-B.json\"\ntest_data = load_test(test_path)\npredictions = predict_answers(model, tokenizer, test_data)\n\nwith open(\"/kaggle/working/Test-B-predicted.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(predictions, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved predictions to Test-B-predicted.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:01:34.299511Z","iopub.execute_input":"2025-09-20T13:01:34.299804Z","iopub.status.idle":"2025-09-20T13:02:14.374973Z","shell.execute_reply.started":"2025-09-20T13:01:34.299784Z","shell.execute_reply":"2025-09-20T13:02:14.374280Z"}},"outputs":[{"name":"stdout","text":"Saved predictions to Test-B-predicted.json\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfrom rouge import Rouge\n\n# Load validation and test-A gold data\nval_data = load_test(\"/kaggle/input/vatika/validation.json\")\ntestA_data = load_test(\"/kaggle/input/vatika/test-A-gold.json\")\n\ndef extract_references(data):\n    refs, qs, ctxs = [], [], []\n    for domain in data[\"domains\"]:\n        for ctx in domain[\"contexts\"]:\n            for qa in ctx[\"qas\"]:\n                refs.append(qa[\"answer\"])\n                qs.append(qa[\"question\"])\n                ctxs.append(ctx[\"context\"])\n    return refs, qs, ctxs\n\ndef get_predictions(qs, ctxs):\n    preds = []\n    model.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    with torch.no_grad():\n        for q, c in zip(qs, ctxs):\n            inputs = tokenizer(q, c, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n            outputs = model(**inputs)\n            start = torch.argmax(outputs.start_logits)\n            end = torch.argmax(outputs.end_logits) + 1\n            tokens = inputs[\"input_ids\"][0][start:end]\n            ans = tokenizer.decode(tokens, skip_special_tokens=True).strip()\n            preds.append(ans)\n    return preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:02:14.375672Z","iopub.execute_input":"2025-09-20T13:02:14.375905Z","iopub.status.idle":"2025-09-20T13:02:15.683304Z","shell.execute_reply.started":"2025-09-20T13:02:14.375887Z","shell.execute_reply":"2025-09-20T13:02:15.682737Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def compute_metrics(references, predictions, debug=False):\n    # F1\n    f1s = []\n    for ref, pred in zip(references, predictions):\n        ref_tokens, pred_tokens = ref.split(), pred.split()\n        common = set(ref_tokens) & set(pred_tokens)\n        if not common:\n            f1s.append(0.0)\n        else:\n            precision = len(common) / len(pred_tokens) if pred_tokens else 0\n            recall = len(common) / len(ref_tokens) if ref_tokens else 0\n            f1s.append(2 * precision * recall / (precision + recall) if precision + recall > 0 else 0)\n    avg_f1 = sum(f1s) / len(f1s)\n\n    # Exact Match\n    em = sum([1 if r.strip() == p.strip() else 0 for r, p in zip(references, predictions)]) / len(references)\n\n    # BLEU (safe)\n    bleu = 0\n    for r, p in zip(references, predictions):\n        if p.strip():  # skip empty predictions\n            bleu += sentence_bleu([r.split()], p.split())\n    bleu /= len(references)\n\n    # ROUGE-L (safe)\n    rouge = Rouge()\n    rouge_scores = []\n    empty_cases = []\n    for i, (r, p) in enumerate(zip(references, predictions)):\n        if not p.strip():  # empty prediction\n            rouge_scores.append(0.0)\n            empty_cases.append((i, r, p))\n        else:\n            rouge_scores.append(rouge.get_scores(p, r)[0]['rouge-l']['f'])\n    rouge_l = sum(rouge_scores) / len(references)\n\n    if debug and empty_cases:\n        print(\"\\nEmpty predictions found in these cases:\")\n        for i, r, p in empty_cases[:10]:  # show first 10 only\n            print(f\"Index {i} | Ref: {r} | Pred: '{p}'\")\n\n    return avg_f1, em, bleu, rouge_l\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:02:15.684018Z","iopub.execute_input":"2025-09-20T13:02:15.684286Z","iopub.status.idle":"2025-09-20T13:02:15.692478Z","shell.execute_reply.started":"2025-09-20T13:02:15.684258Z","shell.execute_reply":"2025-09-20T13:02:15.691600Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"val_refs, val_qs, val_ctxs = extract_references(val_data)\nval_preds = get_predictions(val_qs, val_ctxs)\nval_f1, val_em, val_bleu, val_rouge = compute_metrics(val_refs, val_preds, debug=True)\n\nprint(\"\\n Validation Metrics\")\nprint(f\"F1 Score: {val_f1:.4f}\")\nprint(f\"Exact Match: {val_em:.4f}\")\nprint(f\"BLEU Score: {val_bleu:.4f}\")\nprint(f\"ROUGE-L Score: {val_rouge:.4f}\")\n\n# Test-A Metrics\ntestA_refs, testA_qs, testA_ctxs = extract_references(testA_data)\ntestA_preds = get_predictions(testA_qs, testA_ctxs)\ntestA_f1, testA_em, testA_bleu, testA_rouge = compute_metrics(testA_refs, testA_preds, debug=True)\n\nprint(\"\\n Test-A Metrics\")\nprint(f\"F1 Score: {testA_f1:.4f}\")\nprint(f\"Exact Match: {testA_em:.4f}\")\nprint(f\"BLEU Score: {testA_bleu:.4f}\")\nprint(f\"ROUGE-L Score: {testA_rouge:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:02:15.693124Z","iopub.execute_input":"2025-09-20T13:02:15.693325Z","iopub.status.idle":"2025-09-20T13:03:20.164256Z","shell.execute_reply.started":"2025-09-20T13:02:15.693301Z","shell.execute_reply":"2025-09-20T13:03:20.163531Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 4-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 3-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 2-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"\nEmpty predictions found in these cases:\nIndex 8 | Ref: हाँ, कुरुक्षेत्र कुंड में सुरक्षा व्यवस्थाएँ प्रमुख धार्मिक अवसरों पर सुरक्षा के उचित प्रबंध किए जाते हैं। | Pred: ''\nIndex 21 | Ref: नहीं, दुर्गा कुंड के पास पार्किंग की सुविधा उपलब्ध नहीं है। | Pred: ''\nIndex 26 | Ref: हाँ, पांडव कुंड में स्नान करने से आत्मा की शुद्धि से जीवन के पाप समाप्त होते हैं। | Pred: ''\nIndex 31 | Ref: हाँ, दुर्गा कुंड में नवरात्रि के दौरान विशेष पूजा और दशहरा के मेले का आयोजन होता है। | Pred: ''\nIndex 34 | Ref: दुर्गा कुंड में नवरात्रि का आयोजन 9 दिनों तक चलता है। | Pred: ''\nIndex 35 | Ref: हाँ, क्रीं कुंड तक पहुँचने के लिए बस, ऑटो-रिक्शा और टैक्सी की सुविधा उपलब्ध है। | Pred: ''\nIndex 36 | Ref: हाँ, क्रीं कुंड से जुड़े सार्वजनिक परिवहन मौजूद हैं। इसके अलावा, स्थानीय बस सेवाओं का भी उपयोग किया जा सकता है। | Pred: ''\nIndex 37 | Ref: नहीं, क्रीं कुंड तक पहुँचने के लिए सार्वजनिक परिवहन सबसे अच्छा विकल्प है। | Pred: ''\nIndex 38 | Ref: हाँ, नाग कुआँ में स्नान करने की अनुमति है। | Pred: ''\nIndex 47 | Ref: नहीं, मणिकर्णिका कुंड में दर्शन के लिए प्रवेश शुल्क नहीं है। | Pred: ''\n\n Validation Metrics\nF1 Score: 0.5091\nExact Match: 0.0000\nBLEU Score: 0.3613\nROUGE-L Score: 0.5352\n\nEmpty predictions found in these cases:\nIndex 30 | Ref: हाँ, बकरिया कुंड एक धार्मिक महत्वपूर्ण स्थान है और वहां लोग धार्मिक कार्य करते हैं। | Pred: ''\nIndex 55 | Ref: हाँ, पंचगंगा कुंड तक पहुंचने के लिए ऑटो-रिक्शा, टैक्सी और बस की सुविधा उपलब्ध है, लेकिन उसके आगे लगभग 100 मीटर पैदल जाना होता है। | Pred: ''\nIndex 59 | Ref: हाँ, अगस्त्य कुंड को किसी भी प्रकार की प्रवेश शुल्क या शुल्क की मांग नहीं की जाती है। | Pred: ''\nIndex 63 | Ref: बकरिया कुंड में सुबह के समय पूजा और स्नान के लिए विशेष रूप से शुभ माना जाता है। | Pred: ''\nIndex 75 | Ref: हाँ, श्रद्धालुओं बिना किसी शुल्क के कुरुक्षेत्र कुंड पर प्रवेश कर सकते हैं और देवी की पूजा में भाग ले सकते हैं। | Pred: ''\nIndex 81 | Ref: हाँ, पंचगंगा कुंड क्षेत्र में सड़कों और गलियों की स्थिति दिव्यांगजनों के लिए अनुकूल नहीं है। | Pred: ''\nIndex 82 | Ref: हाँ, पंचगंगा कुंड में भक्तों को दिव्यांगजनों के साथ यात्रा करते समय सावधानी बरतनी चाहिए। | Pred: ''\nIndex 86 | Ref: हाँ, कुरुक्षेत्र कुंड के पास प्राथमिक चिकित्सा और आपातकालीन सेवाएँ धार्मिक यात्रा के दौरान आकस्मिक स्थितियों से निपटने में मदद करती हैं। | Pred: ''\nIndex 89 | Ref: हाँ, आदिकेशव कुंड में सूर्योदय के समय वातावरण बहुत शांत और पवित्र होता है, जो विशेष रूप से ध्यान और पूजा के लिए उपयुक्त होता है। | Pred: ''\nIndex 103 | Ref: हाँ, लोलार्क कुंड में विशेष अवसरों पर सामूहिक दान कार्यक्रम आयोजित किए जाते हैं। | Pred: ''\n\n Test-A Metrics\nF1 Score: 0.4988\nExact Match: 0.0000\nBLEU Score: 0.3496\nROUGE-L Score: 0.5254\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}