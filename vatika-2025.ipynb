{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12977119,"sourceType":"datasetVersion","datasetId":7794557}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ee0c8c94","cell_type":"code","source":"!pip install rouge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:29:45.527439Z","iopub.execute_input":"2025-09-20T13:29:45.527998Z","iopub.status.idle":"2025-09-20T13:29:49.965224Z","shell.execute_reply.started":"2025-09-20T13:29:45.527961Z","shell.execute_reply":"2025-09-20T13:29:49.964371Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}],"execution_count":1},{"id":"342bf17a-6348-469e-a6cf-949195c8c5d8","cell_type":"code","source":"import json\nimport torch\nfrom datasets import Dataset\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForQuestionAnswering,\n    TrainingArguments,\n    Trainer,\n    default_data_collator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:29:49.967561Z","iopub.execute_input":"2025-09-20T13:29:49.967838Z","iopub.status.idle":"2025-09-20T13:30:17.943890Z","shell.execute_reply.started":"2025-09-20T13:29:49.967809Z","shell.execute_reply":"2025-09-20T13:30:17.943181Z"}},"outputs":[{"name":"stderr","text":"2025-09-20 13:30:04.653029: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758375004.833255      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758375004.893041      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"id":"df1d524d-19d6-4dba-8fd8-c2091653f53b","cell_type":"code","source":"def load_vatika_data(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    contexts, questions, answers = [], [], []\n    for domain in data[\"domains\"]:\n        for item in domain[\"contexts\"]:\n            context = item[\"context\"]\n            for qa in item[\"qas\"]:\n                contexts.append(context)\n                questions.append(qa[\"question\"])\n                answers.append(qa[\"answer\"])\n    return {\"context\": contexts, \"question\": questions, \"answers\": answers}\n\ntrain_path = \"/kaggle/input/vatika/train.json\"\ntrain_data = load_vatika_data(train_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:30:17.944891Z","iopub.execute_input":"2025-09-20T13:30:17.945584Z","iopub.status.idle":"2025-09-20T13:30:18.121785Z","shell.execute_reply.started":"2025-09-20T13:30:17.945556Z","shell.execute_reply":"2025-09-20T13:30:18.121013Z"}},"outputs":[],"execution_count":3},{"id":"c6aa7ec9-9a35-4655-ac8b-1147fe481f41","cell_type":"code","source":"def prepare_features(examples, tokenizer):\n    tokenized = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=512,\n        stride=128,\n        return_offsets_mapping=True,\n        padding=\"max_length\"\n    )\n    start_positions, end_positions = [], []\n    for i, offsets in enumerate(tokenized[\"offset_mapping\"]):\n        context = examples[\"context\"][i]\n        answer = examples[\"answers\"][i]\n        answer_start = context.find(answer)\n        answer_end = answer_start + len(answer)\n\n        start = end = 0\n        for idx, (s, e) in enumerate(offsets):\n            if s <= answer_start < e:\n                start = idx\n            if s < answer_end <= e:\n                end = idx\n                break\n        start_positions.append(start)\n        end_positions.append(end)\n\n    tokenized[\"start_positions\"] = start_positions\n    tokenized[\"end_positions\"] = end_positions\n    return tokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:30:18.125743Z","iopub.execute_input":"2025-09-20T13:30:18.125951Z","iopub.status.idle":"2025-09-20T13:30:18.132766Z","shell.execute_reply.started":"2025-09-20T13:30:18.125935Z","shell.execute_reply":"2025-09-20T13:30:18.132049Z"}},"outputs":[],"execution_count":4},{"id":"73e07605-2a91-4f31-9d09-940bef417fad","cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"google/muril-base-cased\")\n\ndataset = Dataset.from_dict(train_data)\ntokenized_dataset = dataset.map(lambda x: prepare_features(x, tokenizer), batched=True)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./muril-vatika\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=8,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    save_strategy=\"no\",\n    logging_strategy=\"no\",\n    report_to=\"none\",\n    disable_tqdm=True\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer,\n    data_collator=default_data_collator\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:30:18.133556Z","iopub.execute_input":"2025-09-20T13:30:18.133787Z","iopub.status.idle":"2025-09-20T14:01:38.243917Z","shell.execute_reply.started":"2025-09-20T13:30:18.133768Z","shell.execute_reply":"2025-09-20T14:01:38.243294Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"680482b5896b40f2978791bceb5fb265"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21f11bf8c1e54b13bef57b2ebe124eab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d22818fa972d4657a92deff8e0b82759"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b8c2b0e36d141b387507901da39ae9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"401ce2c099c64952b4dd37abbe1682f4"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43123e543f7740378aca1c1cedc9e737"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13092 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba7e3ea6fb48436f9fc5189f167c11c4"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/2559973317.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'train_runtime': 1847.8135, 'train_samples_per_second': 14.17, 'train_steps_per_second': 0.886, 'train_loss': 4.844979944101038, 'epoch': 2.0}\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1638, training_loss=4.844979944101038, metrics={'train_runtime': 1847.8135, 'train_samples_per_second': 14.17, 'train_steps_per_second': 0.886, 'train_loss': 4.844979944101038, 'epoch': 2.0})"},"metadata":{}}],"execution_count":5},{"id":"f831a0ca-bbb7-48ac-ad3d-925c37e7154d","cell_type":"code","source":"def load_test(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        return json.load(f)\n\ndef predict_answers(model, tokenizer, test_data):\n    model.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    for domain in test_data[\"domains\"]:\n        for ctx in domain[\"contexts\"]:\n            context = ctx[\"context\"]\n            for qa in ctx[\"qas\"]:\n                question = qa[\"question\"]\n                inputs = tokenizer(\n                    question,\n                    context,\n                    return_tensors=\"pt\",\n                    truncation=True,\n                    max_length=512,\n                    padding=\"max_length\"\n                ).to(device)\n\n                with torch.no_grad():\n                    outputs = model(**inputs)\n                    start = torch.argmax(outputs.start_logits)\n                    end = torch.argmax(outputs.end_logits)\n                    tokens = inputs[\"input_ids\"][0][start:end+1]\n                    answer = tokenizer.decode(tokens, skip_special_tokens=True)\n                    qa[\"answer\"] = answer.strip()\n    return test_data\n\ntest_path = \"/kaggle/input/vatika/test-B.json\"\ntest_data = load_test(test_path)\npredictions = predict_answers(model, tokenizer, test_data)\n\nwith open(\"/kaggle/working/Test-B-predicted.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(predictions, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved predictions to Test-B-predicted.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:01:38.244686Z","iopub.execute_input":"2025-09-20T14:01:38.244912Z","iopub.status.idle":"2025-09-20T14:02:24.994165Z","shell.execute_reply.started":"2025-09-20T14:01:38.244888Z","shell.execute_reply":"2025-09-20T14:02:24.993370Z"}},"outputs":[{"name":"stdout","text":"Saved predictions to Test-B-predicted.json\n","output_type":"stream"}],"execution_count":6},{"id":"71448437-93dc-419f-9c17-93a886ed4bb5","cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfrom rouge import Rouge\n\nval_data = load_test(\"/kaggle/input/vatika/validation.json\")\ntestA_data = load_test(\"/kaggle/input/vatika/test-A-gold.json\")\n\ndef extract_references(data):\n    refs, qs, ctxs = [], [], []\n    for domain in data[\"domains\"]:\n        for ctx in domain[\"contexts\"]:\n            for qa in ctx[\"qas\"]:\n                refs.append(qa[\"answer\"])\n                qs.append(qa[\"question\"])\n                ctxs.append(ctx[\"context\"])\n    return refs, qs, ctxs\n\ndef get_predictions(qs, ctxs):\n    preds = []\n    model.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    with torch.no_grad():\n        for q, c in zip(qs, ctxs):\n            inputs = tokenizer(q, c, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n            outputs = model(**inputs)\n            start = torch.argmax(outputs.start_logits)\n            end = torch.argmax(outputs.end_logits) + 1\n            tokens = inputs[\"input_ids\"][0][start:end]\n            ans = tokenizer.decode(tokens, skip_special_tokens=True).strip()\n            preds.append(ans)\n    return preds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:02:24.995087Z","iopub.execute_input":"2025-09-20T14:02:24.995502Z","iopub.status.idle":"2025-09-20T14:02:25.569633Z","shell.execute_reply.started":"2025-09-20T14:02:24.995470Z","shell.execute_reply":"2025-09-20T14:02:25.569006Z"}},"outputs":[],"execution_count":7},{"id":"572b2d61-28a0-4e76-8ae5-28b12c7d0f40","cell_type":"code","source":"def compute_metrics(references, predictions):\n    # F1\n    f1s = []\n    for ref, pred in zip(references, predictions):\n        ref_tokens, pred_tokens = ref.split(), pred.split()\n        if not pred_tokens:  # empty prediction\n            f1s.append(0)\n            continue\n        common = set(ref_tokens) & set(pred_tokens)\n        if not common:\n            f1s.append(0)\n        else:\n            precision = len(common) / len(pred_tokens)\n            recall = len(common) / len(ref_tokens) if ref_tokens else 0\n            if precision + recall == 0:\n                f1s.append(0)\n            else:\n                f1s.append(2 * precision * recall / (precision + recall))\n    avg_f1 = sum(f1s) / len(f1s)\n\n    # Exact Match\n    em = sum([1 if r.strip() == p.strip() else 0 for r, p in zip(references, predictions)]) / len(references)\n\n    # BLEU\n    bleu = sum([\n        sentence_bleu([r.split()], p.split()) if p.strip() else 0\n        for r, p in zip(references, predictions)\n    ]) / len(references)\n\n    # ROUGE-L (skip empty)\n    rouge = Rouge()\n    rouge_scores = []\n    for r, p in zip(references, predictions):\n        if r.strip() and p.strip():\n            rouge_scores.append(rouge.get_scores(p, r)[0]['rouge-l']['f'])\n        else:\n            rouge_scores.append(0.0)\n    rouge_l = sum(rouge_scores) / len(rouge_scores)\n\n    return avg_f1, em, bleu, rouge_l\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:02:25.570338Z","iopub.execute_input":"2025-09-20T14:02:25.570623Z","iopub.status.idle":"2025-09-20T14:02:25.578091Z","shell.execute_reply.started":"2025-09-20T14:02:25.570599Z","shell.execute_reply":"2025-09-20T14:02:25.577408Z"}},"outputs":[],"execution_count":8},{"id":"689b144c-6f6e-4717-9948-d35e3772c5b7","cell_type":"code","source":"val_refs, val_qs, val_ctxs = extract_references(val_data)\nval_preds = get_predictions(val_qs, val_ctxs)\nval_f1, val_em, val_bleu, val_rouge = compute_metrics(val_refs, val_preds)\n\nprint(\"\\nValidation Metrics\")\nprint(f\"F1 Score: {val_f1:.4f}\")\nprint(f\"Exact Match: {val_em:.4f}\")\nprint(f\"BLEU Score: {val_bleu:.4f}\")\nprint(f\"ROUGE-L Score: {val_rouge:.4f}\")\n\ntestA_refs, testA_qs, testA_ctxs = extract_references(testA_data)\ntestA_preds = get_predictions(testA_qs, testA_ctxs)\ntestA_f1, testA_em, testA_bleu, testA_rouge = compute_metrics(testA_refs, testA_preds)\n\nprint(\"\\n Test-A Metrics\")\nprint(f\"F1 Score: {testA_f1:.4f}\")\nprint(f\"Exact Match: {testA_em:.4f}\")\nprint(f\"BLEU Score: {testA_bleu:.4f}\")\nprint(f\"ROUGE-L Score: {testA_rouge:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:02:25.578829Z","iopub.execute_input":"2025-09-20T14:02:25.579083Z","iopub.status.idle":"2025-09-20T14:03:33.869304Z","shell.execute_reply.started":"2025-09-20T14:02:25.579055Z","shell.execute_reply":"2025-09-20T14:03:33.868648Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 4-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 3-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 2-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"\nValidation Metrics\nF1 Score: 0.4709\nExact Match: 0.0000\nBLEU Score: 0.3343\nROUGE-L Score: 0.4920\n\n Test-A Metrics\nF1 Score: 0.4696\nExact Match: 0.0000\nBLEU Score: 0.3344\nROUGE-L Score: 0.4924\n","output_type":"stream"}],"execution_count":9},{"id":"bbf797f1-b70a-4d77-a6a8-2218ab08750a","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}